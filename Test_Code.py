text = '<p>In probability and statistics, a <b>generative model</b> is a model for generating all values for a phenomenon, both those that can be observed in the world and "target" variables that can only be computed from those observed. By contrast, discriminative models provide a model only for the target variable(s), generating them by analyzing the observed variables. In simple terms, discriminative models infer outputs based on inputs, while generative models generate both inputs and outputs, typically given some hidden parameters.</p>\n<p>Generative models are used in machine learning for either modeling data directly (i.e., modeling observations drawn from a probability density function), or as an intermediate step to forming a conditional probability density function. Generative models are typically probabilistic, specifying a joint probability distribution over observation and target (label) values. A conditional distribution can be formed from a generative model through Bayes\' rule.</p>\n<p>Shannon (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with "representing and speedily is an good"; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc.</p>\n<p>Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express complex relationships between the observed and target variables. They don\'t necessarily perform better than generative models at classification and regression tasks. The two classes are seen as complementary or as different views of the same procedure.</p>\n<p></p>\n\n<p></p>\n<h2><span id="Types">Types</span></h2>\n<h3><span id="Generative_models">Generative models</span></h3>\n<p>Types of generative models are:</p>\n<ul><li>Gaussian mixture model (and other types of mixture model)</li>\n<li>Hidden Markov model</li>\n<li>Probabilistic context-free grammar</li>\n<li>Naive Bayes</li>\n<li>Averaged one-dependence estimators</li>\n<li>Latent Dirichlet allocation</li>\n<li>Restricted Boltzmann machine</li>\n<li>Generative adversarial networks</li>\n</ul><p>If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the <i>true</i> distribution, if the model\'s application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see above), although application-specific details will ultimately dictate which approach is most suitable in any particular case.</p>\n<h3><span id="Discriminative_models">Discriminative models</span></h3>\n<ul><li>Logistic regression,</li>\n<li>Support Vector Machines,</li>\n<li>Maximum Entropy Markov Model,</li>\n<li>Conditional Random Fields,</li>\n<li>Neural Networks</li>\n</ul><h2><span id="Machine_learning">Machine learning</span></h2>\n<p>A generative algorithm models how the data was generated in order to categorize a signal. It asks the question: based on my generation assumptions, which category is most likely to generate this signal? A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal.</p>\n<p>Suppose the input data is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>x</mi>\n        <mo>∈</mo>\n        <mo fence="false" stretchy="false">{</mo>\n        <mn>0</mn>\n        <mo>,</mo>\n        <mn>1</mn>\n        <mo fence="false" stretchy="false">}</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle x\\in \\{0,1\\}}</annotation>\n  </semantics></math></span></span> and the set of labels for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>x</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle x}</annotation>\n  </semantics></math></span></span> is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>y</mi>\n        <mo>∈</mo>\n        <mo fence="false" stretchy="false">{</mo>\n        <mn>0</mn>\n        <mo>,</mo>\n        <mn>1</mn>\n        <mo fence="false" stretchy="false">}</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle y\\in \\{0,1\\}}</annotation>\n  </semantics></math></span></span>. A generative model learns the joint probability distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n        <mo stretchy="false">(</mo>\n        <mi>x</mi>\n        <mo>,</mo>\n        <mi>y</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p(x,y)}</annotation>\n  </semantics></math></span></span> while a discriminative model learns the conditional probability distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n        <mo stretchy="false">(</mo>\n        <mi>y</mi>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mi>x</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p(y|x)}</annotation>\n  </semantics></math></span></span> “probability of y given x”.</p>\n<p>Let\'s try to understand this with an example. Consider the following 4 data points: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mo stretchy="false">(</mo>\n        <mi>x</mi>\n        <mo>,</mo>\n        <mi>y</mi>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <mo fence="false" stretchy="false">{</mo>\n        <mo stretchy="false">(</mo>\n        <mn>0</mn>\n        <mo>,</mo>\n        <mn>0</mn>\n        <mo stretchy="false">)</mo>\n        <mo>,</mo>\n        <mo stretchy="false">(</mo>\n        <mn>0</mn>\n        <mo>,</mo>\n        <mn>0</mn>\n        <mo stretchy="false">)</mo>\n        <mo>,</mo>\n        <mo stretchy="false">(</mo>\n        <mn>1</mn>\n        <mo>,</mo>\n        <mn>0</mn>\n        <mo stretchy="false">)</mo>\n        <mo>,</mo>\n        <mo stretchy="false">(</mo>\n        <mn>1</mn>\n        <mo>,</mo>\n        <mn>1</mn>\n        <mo stretchy="false">)</mo>\n        <mo fence="false" stretchy="false">}</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle (x,y)=\\{(0,0),(0,0),(1,0),(1,1)\\}}</annotation>\n  </semantics></math></span></span></p>\n<p>For above data, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n        <mo stretchy="false">(</mo>\n        <mi>x</mi>\n        <mo>,</mo>\n        <mi>y</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p(x,y)}</annotation>\n  </semantics></math></span></span> will be following:</p>\n<p>while <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n        <mo stretchy="false">(</mo>\n        <mi>y</mi>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mi>x</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p(y|x)}</annotation>\n  </semantics></math></span></span> will be following:</p>\n<p>So, discriminative algorithms try to learn <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n        <mo stretchy="false">(</mo>\n        <mi>y</mi>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mi>x</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p(y|x)}</annotation>\n  </semantics></math></span></span> directly from the data and then try to classify data. On the other hand, generative algorithms try to learn <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n        <mo stretchy="false">(</mo>\n        <mi>x</mi>\n        <mo>,</mo>\n        <mi>y</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p(x,y)}</annotation>\n  </semantics></math></span></span> which can be transformed into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n        <mo stretchy="false">(</mo>\n        <mi>y</mi>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mi>x</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p(y|x)}</annotation>\n  </semantics></math></span></span> later to classify the data. One of the advantages of generative algorithms is that you can use <span><span><math xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n        <mo stretchy="false">(</mo>\n        <mi>x</mi>\n        <mo>,</mo>\n        <mi>y</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p(x,y)}</annotation>\n  </semantics></math></span></span> to generate new data similar to existing data. On the other hand, discriminative algorithms generally give better performance in classification tasks.</p>\n<h2><span id="See_also">See also</span></h2>\n\n<ul><li>Discriminative model</li>\n<li>Graphical model</li>\n</ul><h2><span id="References">References</span></h2>\n\n<h2><span id="Sources">Sources</span></h2>\n<ul><li>Shannon, C.E. (1948) "A Mathematical Theory of Communication", <i>Bell System Technical Journal</i>, vol. 27, pp.\xa0379–423, 623–656, July, October, 1948</li>\n</ul>'
set1df = pd.DataFrame(query('Category: Machine learning'))
wiki_keywords = [
    {'keyword': 'model'},
    {'keyword': 'logistic regression'},
    {'keyword':'accounting software'},
    {'keyword': 'natural language processing'}
]
print(clean_text(text))
display(pd.DataFrame(query('Category: Machine learning')))

