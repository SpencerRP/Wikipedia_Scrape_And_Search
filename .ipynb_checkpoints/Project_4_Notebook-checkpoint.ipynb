{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.6/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run __init__.py\n",
    "\n",
    "def make_query(title):\n",
    "    title = re.sub('\\\\s', '+', re.sub(':','%3A+', title.strip()))\n",
    "    query = 'http://en.wikipedia.org/w/api.php?action=query&format=json&'+\\\n",
    "    'list=categorymembers&cmtitle={}'.format(title)+'&cmlimit=max'\n",
    "    return query\n",
    "\n",
    "def query(title):\n",
    "    r = requests.get(make_query(title)).json()\n",
    "    try:\n",
    "        return r['query']['categorymembers']\n",
    "    except:\n",
    "        print('Error with: ', make_query(title), r)\n",
    "        return {'title':[]}\n",
    "\n",
    "def content_query(pageid):\n",
    "    URL = 'http://en.wikipedia.org/w/api.php?action=query&format=json&pageids={}&prop=extracts&rvprop=content'.format(pageid)\n",
    "    query = requests.get(URL)\n",
    "    return query\n",
    "\n",
    "def get_text(pageid):\n",
    "    try:\n",
    "        pageid = str(int(pageid))\n",
    "        r = content_query(pageid)\n",
    "        text = r.json()['query']['pages'][pageid]['extract']\n",
    "        return text\n",
    "    except:\n",
    "        print(\"{} is a problem with get_text\".format(pageid))        \n",
    "def clean_text(text):\n",
    "    text = re.findall('\\u003E([\\w\\s\\,\\.][^\\u003C]+)(?=\\u003C[\\w\\/])', text)\n",
    "    #find all text between \">\" and \"</\".\n",
    "    text = ' '.join(text) #make a string out of it\n",
    "    text = unicodedata.normalize('NFKD', text) #take unicode like /xao3 and make them normal\n",
    "    text = re.sub('\\\\n', '', re.sub('\\s+', ' ', text)) #get rid of \\n, and condense spaces.\n",
    "    text = re.sub('[\\.,](?=\\w[^\\.])', '. ', text) #Shark.b8 -> Shark. b8; B.C. -> B.C.\n",
    "    \n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    #For some reason I can't simply replace \\' with ', so I'm just getting rid of it all.\n",
    "    #e.g. 'through Bayes\\' rule.Sthrough Bayes\\' rule.S'.replace(\"\\'\",\"'\") works,\n",
    "    #but if you do text = text.replace(...) it doesn't. I don't know why.\n",
    "    \n",
    "    text = re.sub(r\"\\B([A-Z])\", r\" \\1\", text) #or (?<=\\w)([A-Z]). ABaby -> A Baby\n",
    "    text = re.sub('(:)(?=\\w)', ': ', text) #for example:child -> for example: child\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://en.wikipedia.org/w/api.php?action=query&format=json&list=categorymembers&cmtitle=Category%3A+American+ornithological+writers&cmlimit=max\n",
      "[{'pageid': 28885582, 'ns': 0, 'title': 'Alfred Marshall Bailey'}, {'pageid': 48784947, 'ns': 0, 'title': 'Gladys Black'}, {'pageid': 47515963, 'ns': 0, 'title': 'Edward H. Burtt Jr.'}, {'pageid': 4173249, 'ns': 0, 'title': 'Stanley Cobb'}, {'pageid': 32451306, 'ns': 0, 'title': 'William Leon Dawson'}, {'pageid': 8663808, 'ns': 0, 'title': 'Caroline Dormon'}, {'pageid': 4409559, 'ns': 0, 'title': 'Bernd Heinrich'}, {'pageid': 21799405, 'ns': 0, 'title': 'Ralph Hoffmann'}, {'pageid': 22619859, 'ns': 0, 'title': 'Milton N. Hopkins'}, {'pageid': 44565943, 'ns': 0, 'title': 'Richard F. Johnston'}, {'pageid': 3463908, 'ns': 0, 'title': 'Kenn Kaufman'}, {'pageid': 30557049, 'ns': 0, 'title': 'Graceanna Lewis'}, {'pageid': 52638022, 'ns': 0, 'title': 'Lacy Irvine Moffett'}, {'pageid': 332896, 'ns': 0, 'title': 'Roger Tory Peterson'}, {'pageid': 27334521, 'ns': 0, 'title': 'Sewall Pettingill'}, {'pageid': 897230, 'ns': 0, 'title': 'Pamela C. Rasmussen'}, {'pageid': 34821547, 'ns': 0, 'title': 'Chandler Robbins'}, {'pageid': 43225079, 'ns': 0, 'title': 'Don Stap'}, {'pageid': 161523, 'ns': 0, 'title': 'Robert Stroud'}, {'pageid': 49230100, 'ns': 0, 'title': 'Noah Strycker'}, {'pageid': 55841833, 'ns': 0, 'title': 'Thomas Sadler Roberts'}, {'pageid': 52476969, 'ns': 0, 'title': 'William Lay Thompson'}, {'pageid': 15160266, 'ns': 0, 'title': 'Melvin Alvah Traylor Jr.'}, {'pageid': 35937441, 'ns': 0, 'title': 'Lawrence H. Walkinshaw'}, {'pageid': 23345342, 'ns': 0, 'title': 'Scott Weidensaul'}, {'pageid': 1358785, 'ns': 0, 'title': 'Marie Winn'}, {'pageid': 20184272, 'ns': 0, 'title': 'Lawrence Zeleny'}]\n",
      "48784947\n",
      "<p><b>Gladys Bowery \n",
      "Gladys Bowery Black \n"
     ]
    }
   ],
   "source": [
    "example_title = 'Category:American ornithological writers'\n",
    "example_category_query_html = make_query(example_title); print(example_category_query_html)\n",
    "example_cat_members = query(example_title); print(example_cat_members)\n",
    "example_pageid = example_cat_members[1]['pageid']; print(example_pageid)\n",
    "example_text = get_text(str(int(example_pageid))); print(example_text[0:20])\n",
    "example_clean_text = clean_text(example_text); print(example_clean_text[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_search(pickle = False, original_categories = None): #Step 1\n",
    "    if original_categories == None:\n",
    "        original_categories = set(str(x) for x in input().strip().split(','));\n",
    "    else: \n",
    "        original_categories = set(str(x) for x in original_categories.strip().split(','));\n",
    "    print(original_categories)\n",
    "    if pickle: #If I want to pickle something...:\n",
    "        json_list = []\n",
    "        #Make an empty list to use in case I don't want to put stuff in my MongoDB.\n",
    "        #This function will turn the list into a pd.DataFrame and pickle it, if so.\n",
    "    else:\n",
    "        client = pymongo.MongoClient('54.201.49.164', 27016)\n",
    "        client.drop_database('wiki_database')\n",
    "        db_ref = client.wiki_database\n",
    "        coll_ref = db_ref.wiki_text\n",
    "\n",
    "    for original in original_categories:\n",
    "        categories, visited = set([original],), set();\n",
    "        while 1:\n",
    "            if not len(categories):\n",
    "                break\n",
    "            visited.update(categories)\n",
    "            category = categories.pop()\n",
    "            temp_df = pd.DataFrame(query(category))\n",
    "            temp_df.set_index('title', inplace = True)\n",
    "            try: titles = set(temp_df.index)\n",
    "            except: print('The only non-categories are', visited - categories)\n",
    "            categories.update(set(title for title in titles if 'Category:' in title))\n",
    "            titles = titles - categories\n",
    "            for title in titles:\n",
    "                page_id = str(int(temp_df.loc[title, 'pageid']))\n",
    "                text = clean_text(get_text(page_id))\n",
    "                temp_json = {'proximate_category': category,\n",
    "                             'ultimate_category': original,\n",
    "                             'page_name': title,\n",
    "                             'page_id': page_id,\n",
    "                             'text': text,\n",
    "                            }\n",
    "                if pickle: \n",
    "                    json_list.append(temp_json)\n",
    "                else:\n",
    "                    coll_ref.insert_one(temp_json)\n",
    "    if pickle: \n",
    "        pickle_df = pd.DataFrame(json_list)\n",
    "        pickle_df.to_pickle('wiki_clean_text_df.p')\n",
    "    else: \n",
    "        print('Not Pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Category:American ornithological writers'}\n"
     ]
    }
   ],
   "source": [
    "category_search(True, 'Category:American ornithological writers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe():\n",
    "    try:\n",
    "        print('Trying to read db from local pickle.')\n",
    "        df = pd.read_pickle('wiki_clean_text_df.p')\n",
    "        df[0]\n",
    "        return df\n",
    "    except:\n",
    "        print('Failed to read db from local pickle, trying to read from online database.')\n",
    "        try: \n",
    "            client = pymongo.MongoClient('54.201.49.164', 27016)\n",
    "            db_ref = client.wiki_database\n",
    "            coll_ref = db_ref.wiki_text\n",
    "            cursor = coll_ref.find()\n",
    "            all_pages = list(cursor)\n",
    "            df = pd.DataFrame(all_pages)\n",
    "            return df\n",
    "        except:\n",
    "            print('Database inaccessible. Please try again when it is available.')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read db from local pickle.\n",
      "Failed to read db from local pickle, trying to read from online database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_name</th>\n",
       "      <th>proximate_category</th>\n",
       "      <th>text</th>\n",
       "      <th>ultimate_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a2cb362ccd10800ce71f7a3</td>\n",
       "      <td>27334521</td>\n",
       "      <td>Sewall Pettingill</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "      <td>Olin Sewall Pettingill, Jr. , (October 30, 190...</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a2cb363ccd10800ce71f7a4</td>\n",
       "      <td>3463908</td>\n",
       "      <td>Kenn Kaufman</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "      <td>Kenn Kaufman (born 1954) is an American author...</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a2cb363ccd10800ce71f7a5</td>\n",
       "      <td>30557049</td>\n",
       "      <td>Graceanna Lewis</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "      <td>Graceanna Lewis (August 3, 1821 – February 25,...</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a2cb363ccd10800ce71f7a6</td>\n",
       "      <td>43225079</td>\n",
       "      <td>Don Stap</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "      <td>Don Stap (born 1949) is an American author who...</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a2cb363ccd10800ce71f7a7</td>\n",
       "      <td>44565943</td>\n",
       "      <td>Richard F. Johnston</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "      <td>Richard Fourness Johnston (July 27, 1925 – Nov...</td>\n",
       "      <td>Category:American ornithological writers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id   page_id            page_name  \\\n",
       "0  5a2cb362ccd10800ce71f7a3  27334521    Sewall Pettingill   \n",
       "1  5a2cb363ccd10800ce71f7a4   3463908         Kenn Kaufman   \n",
       "2  5a2cb363ccd10800ce71f7a5  30557049      Graceanna Lewis   \n",
       "3  5a2cb363ccd10800ce71f7a6  43225079             Don Stap   \n",
       "4  5a2cb363ccd10800ce71f7a7  44565943  Richard F. Johnston   \n",
       "\n",
       "                         proximate_category  \\\n",
       "0  Category:American ornithological writers   \n",
       "1  Category:American ornithological writers   \n",
       "2  Category:American ornithological writers   \n",
       "3  Category:American ornithological writers   \n",
       "4  Category:American ornithological writers   \n",
       "\n",
       "                                                text  \\\n",
       "0  Olin Sewall Pettingill, Jr. , (October 30, 190...   \n",
       "1  Kenn Kaufman (born 1954) is an American author...   \n",
       "2  Graceanna Lewis (August 3, 1821 – February 25,...   \n",
       "3  Don Stap (born 1949) is an American author who...   \n",
       "4  Richard Fourness Johnston (July 27, 1925 – Nov...   \n",
       "\n",
       "                          ultimate_category  \n",
       "0  Category:American ornithological writers  \n",
       "1  Category:American ornithological writers  \n",
       "2  Category:American ornithological writers  \n",
       "3  Category:American ornithological writers  \n",
       "4  Category:American ornithological writers  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>110</th>\n",
       "      <th>116</th>\n",
       "      <th>12</th>\n",
       "      <th>125</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>written</th>\n",
       "      <th>wrote</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "      <th>yorks</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>zoological</th>\n",
       "      <th>zoology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sewall Pettingill</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043481</td>\n",
       "      <td>0.070949</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kenn Kaufman</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099502</td>\n",
       "      <td>0.027060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Graceanna Lewis</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021824</td>\n",
       "      <td>0.071221</td>\n",
       "      <td>0.068437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Don Stap</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028308</td>\n",
       "      <td>0.181341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard F. Johnston</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      00       000   10  100        11  110  116        12  \\\n",
       "page_name                                                                    \n",
       "Sewall Pettingill    0.0  0.000000  0.0  0.0  0.105056  0.0  0.0  0.000000   \n",
       "Kenn Kaufman         0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  0.000000   \n",
       "Graceanna Lewis      0.0  0.000000  0.0  0.0  0.026365  0.0  0.0  0.000000   \n",
       "Don Stap             0.0  0.044229  0.0  0.0  0.000000  0.0  0.0  0.000000   \n",
       "Richard F. Johnston  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  0.056702   \n",
       "\n",
       "                     125   13    ...      written  wrote      year     years  \\\n",
       "page_name                        ...                                           \n",
       "Sewall Pettingill    0.0  0.0    ...     0.000000    0.0  0.043481  0.070949   \n",
       "Kenn Kaufman         0.0  0.0    ...     0.000000    0.0  0.099502  0.027060   \n",
       "Graceanna Lewis      0.0  0.0    ...     0.000000    0.0  0.021824  0.071221   \n",
       "Don Stap             0.0  0.0    ...     0.053456    0.0  0.000000  0.028308   \n",
       "Richard F. Johnston  0.0  0.0    ...     0.000000    0.0  0.000000  0.000000   \n",
       "\n",
       "                         york  yorks     young  younger  zoological   zoology  \n",
       "page_name                                                                      \n",
       "Sewall Pettingill    0.090900    0.0  0.000000      0.0         0.0  0.000000  \n",
       "Kenn Kaufman         0.000000    0.0  0.000000      0.0         0.0  0.000000  \n",
       "Graceanna Lewis      0.068437    0.0  0.000000      0.0         0.0  0.026365  \n",
       "Don Stap             0.181341    0.0  0.044229      0.0         0.0  0.000000  \n",
       "Richard F. Johnston  0.000000    0.0  0.000000      0.0         0.0  0.101461  \n",
       "\n",
       "[5 rows x 1410 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tfidf_matrix_df(df, tfidf_vectorizer = None, min_df = 2, stop_words = 'english'):\n",
    "    if tfidf_vectorizer == None: \n",
    "        tfidf_vectorizer = TfidfVectorizer(min_df = min_df, stop_words = stop_words)\n",
    "        term_matrix_sps = tfidf_vectorizer.fit_transform(df.text)\n",
    "    else: term_matrix_sps = tfidf_vectorizer.transform(df.text);\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df = min_df, stop_words = stop_words)\n",
    "    term_matrix_sps = tfidf_vectorizer.fit_transform(df.text)\n",
    "    term_matrix_df = pd.DataFrame(term_matrix_sps.toarray(),\n",
    "                                  index=df.page_name,\n",
    "                                  columns=tfidf_vectorizer.get_feature_names(),\n",
    "                                 )\n",
    "    return term_matrix_df\n",
    "make_tfidf_matrix_df(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_match(search): #Step 2\n",
    "    #Establish a dataframe from either a pickled version or pulled from MongoDB.\n",
    "    df = get_dataframe()\n",
    "    #Make a TFIDF Matrix from the database established above.\n",
    "    #document_term_matrix_df = make_tfidf_matrix_df(df)\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df = 2, stop_words = 'english')\n",
    "    term_matrix_sps = tfidf_vectorizer.fit_transform(df.text)\n",
    "    document_term_matrix_df = pd.DataFrame(term_matrix_sps.toarray(),\n",
    "                                  index=df.page_name,\n",
    "                                  columns=tfidf_vectorizer.get_feature_names(),\n",
    "                                 )\n",
    "    if type(search) != str:\n",
    "        #if search != a string, there is presumably more than one search being performed.\n",
    "        #In such a case, the dataframe will have more than one search.\n",
    "        search_list = [{'search':sample} for sample in list(search)]\n",
    "        #idk why I picked sample, but I didn't want to use the plural \"searches.\"\n",
    "    else:\n",
    "        #if search = 1 string, the dataframe will have only one search.\n",
    "        search_list = list([{'search':search}])\n",
    "    \n",
    "    #Establish a keyword dataframe from the \"search\" put in above.\n",
    "    search_df = pd.DataFrame(search_list)\n",
    "    vectorized_search = tfidf_vectorizer.transform(search_df.search)\n",
    "    search_df = pd.DataFrame(vectorized_search.toarray(),\n",
    "                             index = search_df.search,\n",
    "                             columns=tfidf_vectorizer.get_feature_names())\n",
    "    return_df = pd.DataFrame() #make an empty DataFrame\n",
    "\n",
    "    for search_instance in search_df.index: #go through the search terms and do an SVD Cosine Similarity to them.\n",
    "        random_search_df = search_df.loc[search_instance]\n",
    "        #display(random_search_df)\n",
    "        \n",
    "        dtm_with_search_term = document_term_matrix_df.append(random_search_df)\n",
    "        #display(dtm_with_search_term)\n",
    "        #Make an SVD and apply it to dtm_with_search_term\n",
    "        n_components = len(dtm_with_search_term.index);\n",
    "        SVD = TruncatedSVD(n_components)\n",
    "        component_names = ['component_' + str(i+1) for i in range(n_components)]\n",
    "        \n",
    "        svd_matrix = SVD.fit_transform(dtm_with_search_term)\n",
    "        \n",
    "        svd_df = pd.DataFrame(svd_matrix,\n",
    "                              index=dtm_with_search_term.index,\n",
    "                              columns=component_names,\n",
    "                             )\n",
    "    \n",
    "        #Find and sort the cosine similarity for the search term.\n",
    "        search_term_svd_vector = svd_df.loc[search_instance]\n",
    "        svd_df['cosine_sim'] = cosine_similarity(svd_df, search_term_svd_vector)\n",
    "        svd_df.drop(search_instance, inplace = True)\n",
    "        #display(svd_df)\n",
    "        \n",
    "        #Take the five most correlated results and put them in a df w/ the initial search term.\n",
    "        temp_df = svd_df[['cosine_sim']].sort_values('cosine_sim', ascending=False).head(5)\n",
    "        temp_df['search'] = search_instance\n",
    "        #display(temp_df)\n",
    "        return_df = pd.concat([return_df, temp_df]) #Add it to the main dataframe we're returning.\n",
    "        \n",
    "    #Return a df w/ the 5 most correlated pages per search.\n",
    "    return return_df.head(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read db from local pickle.\n",
      "Failed to read db from local pickle, trying to read from online database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosine_sim</th>\n",
       "      <th>search</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alfred Marshall Bailey</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Alfred Marshall Bailey (February 18, 1894 – Fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cosine_sim  \\\n",
       "page_name                            \n",
       "Alfred Marshall Bailey         1.0   \n",
       "\n",
       "                                                                   search  \n",
       "page_name                                                                  \n",
       "Alfred Marshall Bailey  Alfred Marshall Bailey (February 18, 1894 – Fe...  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_match(clean_text(get_text(query('Category:American ornithological writers')[0]['pageid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_recommendation(page_text): #Step 3, I think??\n",
    "    df = get_dataframe() #Acquire a DataFrame made previously.\n",
    "    #Find the \"nearest neighbors,\" 5 pages which have the highest correlation with the target.\n",
    "    nearest_neighbors = page_match(page_text)\n",
    "    \n",
    "    #Make a mask to get rows of the neighbors from the df.\n",
    "    is_a_neighbor = [title in nearest_neighbors.index for title in df['page_name'].values]\n",
    "    neighbors = df[is_a_neighbor]\n",
    "    neighbors_categories = neighbors.ultimate_category\n",
    "    #Make a dictionary with the count of each unique neighbor.\n",
    "    #If we had 2 neighbors ML & BS, the dict might look like [{ML:3}, {BS:2}]\n",
    "    neighbor_count = {neighbor: sum(neighbors_categories == neighbor) for neighbor in set(neighbors_categories)}\n",
    "    \n",
    "    #Return the most common category possessed by the neighbors.\n",
    "    #Since ML has a count of 3, it will be returned.\n",
    "    most_common = None\n",
    "    for new in neighbor_count.keys():\n",
    "        if most_common == None: most_common = new\n",
    "        if (neighbor_count[most_common] < neighbor_count[new]):\n",
    "            most_common = new\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read db from local pickle.\n",
      "Failed to read db from local pickle, trying to read from online database.\n",
      "Trying to read db from local pickle.\n",
      "Failed to read db from local pickle, trying to read from online database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Category:American ornithological writers'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_recommendation(clean_text(get_text(query('Category:American ornithological writers')[0]['pageid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Make it so that it doesn't pull the entire database.\n",
    "#The problem is that the tfidf needs a whole database - i.e. more than one document.\n",
    "#A possible method would be: Seeing if I can do the tf & idf separately. i.e., \n",
    "#1) Make term frequency vectors and store them in the database in addition to/instead of the text \n",
    "#2) Pick a # of samples of tf's from the DB, and sum the tf's to make a df.\n",
    "#3) Discard columns with a df of < or > a specified number.\n",
    "#4) Run the resultant tfidf matrix through an SVD. \n",
    "#5) Keep the best 5 and add their term frequency vectors to the next sample of 100.\n",
    "#6) Repeat from 2 until the DB has been exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
